---
title: "Analyzing Major Crime Reports in Toronto over the years"
author: Allen Uy
thanks: "Code and data are available at: https://github.com/varygx/TorontoMajorCrime"
date: today
date-format: long
abstract: Prototype of Term Paper 1. Reports of major crime in Toronto reached a peak in 2023. Based on analysis of data from OpenDataToronto there has been a steady increase of major crime reports since data collection in 2014.
format: pdf
number-sections: true
bibliography: references.bib
---

```{r}
#| include: false
#| warning: false
#| message: false
#### Workspace setup ####
library("tidyverse")
library("janitor")
library("opendatatoronto")
```


# Introduction

Basically a single Quarto doc of my scripts I'm likely using with modification for Term Paper 1 for Mini-essay #2 for easy code review. Major crime indicators are listed as: assault, auto theft, break and enter, homicide, robbery, sexual violation, and theft over.

The remainder of this paper is structured as follows. @sec-data....

# Data {#sec-data}


```{r}
#### Simulate data ####
set.seed(42)

sim_data <- tibble(
  report_id = c(1:100),
  report_year = sample(2013:2023, 100, replace=TRUE)
)
```

```{r, include=FALSE}
#### Download data ####
package <- show_package("major-crime-indicators")

resources <- list_package_resources("major-crime-indicators")

datastore_resources <- filter(resources, tolower(format) %in% c('csv', 'geojson'))

data <- filter(datastore_resources, row_number()==2) |> get_resource()

#### Save data ####

write_csv(data, "../../inputs/data/raw_data.csv")
```

```{r}
#### Clean data ####
raw_data <- read_csv("../../inputs/data/raw_data.csv")

cleaned_data <- clean_names(raw_data)

cleaned_data <- cleaned_data |> select("report_year")

#### Save data ####
write_csv(cleaned_data, "../../outputs/data/analysis_data.csv")
```

```{r}
#### Test data ####
cleaned_data <- read_csv("../../outputs/data/analysis_data.csv")
cleaned_data$report_year |> min() == 2014
cleaned_data$report_year |> max() == 2023
```


# Results


```{r}
#### Visualize data ####
cleaned_data |> ggplot(aes(x=report_year)) +
  geom_bar() +
  theme_minimal() +
  labs(x = "Year", y = "Number of Reports")
```


We can see a steady rise in reports from 2014 to 2019 with a sudden decrease in 2020. This lines up with the COVID pandemic and gives a plausible explanation as less people were likely to be outside. Below is a graph that extrapolates data for 2020 to 2022 based on a linear model fitted to 2014 to 2019 data.


```{r}
year_count <- cleaned_data |> count(report_year)
pre_covid <- year_count |> filter(report_year <= 2019)
fit <- lm(n ~ report_year, data = pre_covid)

predicted_years <- tibble(report_year = c(2020:2022))
predicted_counts <- predict(fit, newdata=predicted_years, type="response")
predicted_data <- tibble(report_year = predicted_years$report_year, n=predicted_counts)

extrapolated_data <- year_count |>
  mutate(n = ifelse(report_year %in% predicted_data$report_year, predicted_data$n, n))

extrapolated_data |> ggplot(aes(x=report_year, y=n)) +
  geom_bar(stat="identity") +
  theme_minimal() +
  labs(x = "Year", y = "Number of Reports")
```


\newpage

# References


```{r, include=FALSE, echo=FALSE}
citation()
citation("tidyverse")
citation("janitor")
citation("opendatatoronto")
```

